{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XdKtb8Zuserg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np \n",
    "import random\n",
    "import time \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Start with vgg19\n",
    "from tensorflow.keras.applications import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "aSI_e9cXwuwy",
    "outputId": "0bd80b74-4dd5-4a51-c28b-37a43661bf91"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "    print('and then re-execute this cell.')\n",
    "else:\n",
    "    print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bSedF9rSwxBp",
    "outputId": "0379d6f7-1a1d-4e6d-a45c-7f37c217db8a"
   },
   "outputs": [],
   "source": [
    "try: # detect TPUs\n",
    "    # detect and init the TPU\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\n",
    "    # instantiate a distribution strategy\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError: # detect GPUs\n",
    "    #strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n",
    "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "IMAGEDIR = 'ParentImages/'\n",
    "OUTDIR = 'TriImages/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JemWzyojPynP"
   },
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)\n",
    "\n",
    "def load_img_x(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    #resize\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    img = img[tf.newaxis, :]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "mpXpewumuAtN",
    "outputId": "fb69c0a3-ce25-44ed-d73b-fba995c76aff"
   },
   "outputs": [],
   "source": [
    "# this is the primary image to explore\n",
    "content_path = IMAGEDIR+'Bacchus.jpg'\n",
    "\n",
    "content = load_img_x(content_path)\n",
    "tnew = vgg19.preprocess_input(content*255)\n",
    "tnew = tf.image.resize(tnew, (224, 224))\n",
    "# first test full VGG19\n",
    "model = vgg19.VGG19(include_top=True, weights='imagenet')\n",
    "probs = model(tnew)\n",
    "probs.shape\n",
    "top_5 = vgg19.decode_predictions(probs.numpy())[0]\n",
    "[(class_name, prob) for (number, class_name, prob) in top_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Fsy3iPcutcH"
   },
   "outputs": [],
   "source": [
    "# well, he is a welshie, but we can forgive mistaking him for an airedale\n",
    "model = vgg19.VGG19(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6TDxgEyPVIh9"
   },
   "outputs": [],
   "source": [
    "# loss functions\n",
    "## content loss\n",
    "def mat_loss(target, combo, scale = 1):\n",
    "    # this is just pixel distance (squared)\n",
    "    return K.sum(K.square(target-combo))/scale \n",
    "\n",
    "\"\"\" The gram matrix captures the style of an image.\n",
    "    In essence it takes a pixel image h x w x n_f, where h and w are the height and width of \n",
    "    the image, and converts it into a n_f x n_f matrix that is a measure of may many of the \n",
    "    layer features have been represented in that image.  Minimizing the gram matrix distances \n",
    "    between the style and combo will be this loss. \n",
    "\"\"\"\n",
    "def gram_matrix(x):\n",
    "    return tf.linalg.einsum('bijc,bijd->bcd', x, x)\n",
    "\n",
    "## style loss\n",
    "## in practice, style loss is computed using mat_loss above\n",
    "def style_loss(style, combo):\n",
    "    gram_sty = gram_matrix(style)\n",
    "    gram_com = gram_matrix(combo)\n",
    "    return K.sum(K.square(gram_sty-gram_com))\n",
    "\n",
    "## total variational loss\n",
    "## encourages pixel continuity in the combo image\n",
    "def variational_loss(combo, diag_weight = 0.5):\n",
    "    # this is just pixel distance (squared)\n",
    "    A = K.square(combo[:,:-1,1:,:] - combo[:,1:,1:,:])\n",
    "    B = K.square(combo[:,1:,:-1,:] - combo[:,1:,1:,:])\n",
    "    C = K.square(combo[:,:-1,:-1,:] - combo[:,1:,1:,:])\n",
    "    D = K.square(combo[:,:-1,1:,:] - combo[:,1:,:-1,:])\n",
    "    # several sources raise this to the 1.25 power - I have no idea why and have been unable\n",
    "    # to locate an original source\n",
    "    return K.sum(A+B+diag_weight*(C+D))\n",
    "\n",
    "# clip to float 0-1\n",
    "def clip_f(image):\n",
    "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triple style transfer\n",
    "Transfer the coarse features from one souce image, the fine features from another source, and colors from a third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "pjTeE3A6wHtC",
    "outputId": "063a0119-32ca-449c-e816-c539957abf99"
   },
   "outputs": [],
   "source": [
    "# what are the vgg19 layer names\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTrxqQrhvwCM"
   },
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name,layer.output) for layer in model.layers])\n",
    "fine_style_layers = ['block1_conv1','block1_conv2',]\n",
    "mid_style_layers = ['block2_conv2','block3_conv2','block3_conv4','block4_conv1']\n",
    "coarse_style_layers = ['block4_conv2','block4_conv3','block5_conv1']\n",
    "content_layers = ['block5_conv3']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_fine_style_layers = len(fine_style_layers)\n",
    "num_mid_style_layers = len(mid_style_layers)\n",
    "num_coarse_style_layers = len(coarse_style_layers)\n",
    "\n",
    "# get untrainable vgg19 for layer extraction\n",
    "def vgg_layers(layer_names):\n",
    "    vgg = vgg19.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "    model = Model([vgg.input], outputs)\n",
    "    return model\n",
    "\n",
    "# this is the heavy lifter (adapted from tf website)\n",
    "class TriNeuralStyleTransferModel(Model):\n",
    "    def __init__(self, fine_style_layers, mid_style_layers, \n",
    "                 coarse_style_layers, content_layers):\n",
    "        super(TriNeuralStyleTransferModel, self).__init__()\n",
    "        self.vgg = vgg_layers(fine_style_layers + mid_style_layers + \n",
    "                              coarse_style_layers + content_layers)\n",
    "        self.fine_style_layers = fine_style_layers\n",
    "        self.mid_style_layers = mid_style_layers\n",
    "        self.coarse_style_layers = coarse_style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.num_fine_style_layers = len(fine_style_layers)\n",
    "        self.num_mid_style_layers = len(mid_style_layers)\n",
    "        self.num_coarse_style_layers = len(coarse_style_layers)\n",
    "        self.num_content_layers = len(content_layers)\n",
    "        self.vgg.trainable = False\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #float input in [0,1]\n",
    "        inputs = inputs*255.0\n",
    "        preprocessed_input = vgg19.preprocess_input(inputs)\n",
    "        outputs = self.vgg(preprocessed_input)\n",
    "        nl1 = self.num_fine_style_layers\n",
    "        nl2 = nl1 + self.num_mid_style_layers\n",
    "        nl3 = nl2 + self.num_coarse_style_layers\n",
    "        fine_style_outputs, mid_style_outputs = (outputs[:nl1], outputs[nl1:nl2])\n",
    "        coarse_style_outputs, content_outputs = (outputs[nl2:nl3], outputs[nl3:])\n",
    "\n",
    "        # get gram matrix for style, combine with combo image later\n",
    "        fine_style_outputs = [gram_matrix(f_style_output)\n",
    "                        for f_style_output in fine_style_outputs]\n",
    "\n",
    "        mid_style_outputs = [gram_matrix(m_style_output)\n",
    "                        for m_style_output in mid_style_outputs]\n",
    "\n",
    "        coarse_style_outputs = [gram_matrix(c_style_output)\n",
    "                        for c_style_output in coarse_style_outputs]\n",
    "\n",
    "        content_dict = {content_name:value \n",
    "                        for content_name, value \n",
    "                        in zip(self.content_layers, content_outputs)}\n",
    "\n",
    "        fine_style_dict = {style_name:value\n",
    "                    for style_name, value\n",
    "                    in zip(self.fine_style_layers, fine_style_outputs)}\n",
    "\n",
    "        mid_style_dict = {style_name:value\n",
    "                    for style_name, value\n",
    "                    in zip(self.mid_style_layers, mid_style_outputs)}\n",
    "\n",
    "        coarse_style_dict = {style_name:value\n",
    "                    for style_name, value\n",
    "                    in zip(self.coarse_style_layers, coarse_style_outputs)}\n",
    "    \n",
    "        return {'content':content_dict, 'fstyle':fine_style_dict, \n",
    "                'mstyle':mid_style_dict, 'cstyle':coarse_style_dict }\n",
    "\n",
    "# define the extractor model\n",
    "tri_extractor = TriNeuralStyleTransferModel(fine_style_layers, mid_style_layers,\n",
    "                                            coarse_style_layers, content_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYwV4amExJqF"
   },
   "outputs": [],
   "source": [
    "def update_weight_dictionary(outs,weight_dict,var_factor,regulator,meanofall):\n",
    "    net_mean=0\n",
    "    ix = 0\n",
    "    for name, output in sorted(outs.items()):\n",
    "        xr = output.numpy().shape\n",
    "        n_c = 1\n",
    "        for x in xr:\n",
    "            n_c *= x \n",
    "        weight = output.numpy().mean()**2 * n_c / var_factor\n",
    "        net_mean += output.numpy().mean()\n",
    "        ix+=1\n",
    "        if weight == 0: \n",
    "            weight = regulator / var_factor\n",
    "        weight_dict[name] = weight\n",
    "    if meanofall:\n",
    "        for name, output in sorted(outs.items()):\n",
    "            xr = output.numpy().shape\n",
    "            n_c = 1\n",
    "            for x in xr:\n",
    "                n_c *= x \n",
    "            weight_dict[name] = netmean**2 * n_c / (var_factor * ix**2)\n",
    "\n",
    "def get_mean_weights_tri(fs_out, ms_out, cs_out, c_out, combo_image, \n",
    "                         regulator=0.1,meanofall=False):\n",
    "    # compute the mean of style, content and var, and use that to weight the values\n",
    "    #size of combo image\n",
    "    xshape = tf.shape(combo_image)\n",
    "    dof = np.float32(xshape[-3]*xshape[-2]*xshape[-1])\n",
    "    # this is the factor against which all others will be normalized\n",
    "    var_factor = variational_loss(combo_image)/dof + 1e-8\n",
    "    # define the dictionary\n",
    "    weight_dict = {\"variation\": 1}\n",
    "    # compute fstyle weights\n",
    "    update_weight_dictionary(fs_out,weight_dict,var_factor,regulator,meanofall)\n",
    "    update_weight_dictionary(ms_out,weight_dict,var_factor,regulator,meanofall)\n",
    "    update_weight_dictionary(cs_out,weight_dict,var_factor,regulator,meanofall)\n",
    "    update_weight_dictionary(c_out,weight_dict,var_factor,regulator,meanofall)\n",
    "    # compute content weights\n",
    "    return weight_dict\n",
    "\n",
    "# this is our master function: path it a content and style image, and enjoy \n",
    "def build_tri_image(content_path,fine_style_path,mid_style_path,coarse_style_path, \n",
    "                    weights = None, powers = None, abort_thresh = 0.0001, \n",
    "                    beta1 = 0.99, beta2 = 0.999, lr = 0.01, epochs=10, \n",
    "                    useMeanWeighting = False, contentStart = True):\n",
    "    c_w,fs_w,ms_w,cs_w,v_w = 1,1,1,1,1\n",
    "    c_p,fs_p,ms_p,cs_p,v_p = 1,1,1,1,1\n",
    "    if weights is not None:\n",
    "        [c_w,fs_w,ms_w,cs_w,v_w]=weights\n",
    "    if powers is not None:\n",
    "        [c_p,fs_p,ms_p,cs_p,v_p]=powers\n",
    "\n",
    "    content_image = load_img_x(content_path)\n",
    "    xshape = tf.shape(content_image)\n",
    "    dof = np.float32(xshape[-3]*xshape[-2]*xshape[-1])\n",
    "    fstyle_image = tf.image.resize(load_img_x(fine_style_path),(xshape[1],xshape[2]))\n",
    "    mstyle_image = tf.image.resize(load_img_x(mid_style_path),(xshape[1],xshape[2]))\n",
    "    cstyle_image = tf.image.resize(load_img_x(coarse_style_path),(xshape[1],xshape[2]))\n",
    "\n",
    "    # Extract base image vgg outputs only once - they won't change \n",
    "    fstyle_outs = tri_extractor(fstyle_image)['fstyle']\n",
    "    mstyle_outs = tri_extractor(mstyle_image)['mstyle']\n",
    "    cstyle_outs = tri_extractor(cstyle_image)['cstyle']\n",
    "    content_outs = tri_extractor(content_image)['content']\n",
    "    combo_image = tf.Variable(content_image)\n",
    "    \n",
    "    if contentStart == False:\n",
    "        combo_image = tf.Variable(tf.random.uniform(shape = xshape))\n",
    "\n",
    "    weight_dict = get_mean_weights_tri(fstyle_outs, mstyle_outs, cstyle_outs,\n",
    "                                        content_outs,combo_image)\n",
    "    if useMeanWeighting:\n",
    "        weight_dict = get_mean_weights_tri(fstyle_outs, mstyle_outs, cstyle_outs,\n",
    "                                            content_outs,combo_image,meanofall=True)\n",
    "\n",
    "\n",
    "    # note: insanely sensitive to hyperparameter choices\n",
    "    # the original paper uses L-BFGS\n",
    "    opt = tf.optimizers.Adam(learning_rate=lr, beta_1=beta1, beta_2=beta2, epsilon=1e-7)\n",
    "    \n",
    "    def total_loss(combo):\n",
    "        # using adam, this won't matter, if SGD, dial it\n",
    "        scale = 1\n",
    "        # get the style and content for the combined image\n",
    "        outputs = tri_extractor(combo)\n",
    "        fstyle_outputs = outputs['fstyle']\n",
    "        mstyle_outputs = outputs['mstyle']\n",
    "        cstyle_outputs = outputs['cstyle']\n",
    "        content_outputs = outputs['content']\n",
    "\n",
    "        # compute fine style loss on all fine style layers\n",
    "        fstyle_loss = tf.add_n([mat_loss(fstyle_outputs[name],fstyle_outs[name], \n",
    "                                         scale=weight_dict[name])\n",
    "                                for name in fstyle_outputs.keys()])\n",
    "        # compute mid style loss on all fine style layers\n",
    "        mstyle_loss = tf.add_n([mat_loss(mstyle_outputs[name],mstyle_outs[name], \n",
    "                                         scale=weight_dict[name])\n",
    "                                for name in mstyle_outputs.keys()])\n",
    "        # compute coarse style loss on all coarse style layers\n",
    "        cstyle_loss = tf.add_n([mat_loss(cstyle_outputs[name],cstyle_outs[name], \n",
    "                                         scale=weight_dict[name])\n",
    "                                for name in cstyle_outputs.keys()])\n",
    "        # compute content loss on all content layers\n",
    "        content_loss = tf.add_n([mat_loss(content_outputs[name],content_outs[name], \n",
    "                                          scale=weight_dict[name])\n",
    "                                    for name in content_outputs.keys()])\n",
    "        # compute variational loss on the combo image\n",
    "        var_loss = variational_loss(combo) / dof\n",
    "        # join all losses using specified weights and powers\n",
    "        loss = fs_w * (fstyle_loss ** fs_p) + ms_w * (mstyle_loss ** ms_p) + \\\n",
    "            cs_w * (cstyle_loss ** cs_p) + c_w * (content_loss ** c_p) + \\\n",
    "            v_w * (var_loss ** v_p)\n",
    "        return scale * loss\n",
    "\n",
    "    #@tf.function()\n",
    "    def train_step(im):\n",
    "        # GradientTape context tracks coputations for gradient back-prop\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = total_loss(im)\n",
    "        grad = tape.gradient(loss, im)\n",
    "        # using Adam here (perhaps recklessly)\n",
    "        opt.apply_gradients([(grad, im)])\n",
    "        # clip back to [0-1] range\n",
    "        im.assign(clip_f(im))\n",
    "        return loss\n",
    "\n",
    "    start = time.time()\n",
    "    steps_per_epoch = 50\n",
    "    step = 0\n",
    "    losses = []\n",
    "    # copy old combo image for early loop exit\n",
    "    oldim = tf.Variable(combo_image)\n",
    "    # epoch loop\n",
    "    for n in range(epochs):\n",
    "        for m in range(steps_per_epoch):\n",
    "            step += 1\n",
    "            losses.append(train_step(combo_image))\n",
    "            print(\".\", end='')\n",
    "        # clear output and display tensoe\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(tensor_to_image(combo_image))\n",
    "        #print(mat_loss(oldim,combo_image)/dof)\n",
    "        # if image is hardly changing, break loop (abort_thresh ~ 1e-4 is good)\n",
    "        # if using a low lr, decrease or set to zero\n",
    "        if mat_loss(oldim,combo_image)/dof < abort_thresh:\n",
    "            break;\n",
    "        # set for next rough\n",
    "        oldim =  tf.Variable(combo_image)\n",
    "    end = time.time()\n",
    "    print(\"Total time: {:.1f}\".format(end-start))\n",
    "    return combo_image, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQ36OsVYVwX5"
   },
   "source": [
    "# Generation\n",
    "\n",
    "lr, Beta1, and fs,cs,c,v_w,p are the important paramters  \n",
    "This is a bit more finicky than the single transfer - some combinations don't work well, also what we suspect is fine (colors, small patterns) vs coarse (shapes, larger patterns) doesn't always work like expected   \n",
    "Basic plan:  \n",
    "1) leave lr / beta_1 at defaults, increasing lr or beta_1 can blur out patches, takes longer if decreased  \n",
    "2) leave v_w and v_p at 1 unless image is too blurry or pixelated (if pixelated, up to 10, if still bad, this is likely a consequence of the pairing)  \n",
    "3) c_w = 0, c_p = 1  \n",
    "4) fs_p, ms_p, cs_p = 1  \n",
    "5) fs_w, ms_w, cs_w = 1, if we want to in/decrease coarse vs mid vs fine, we can move these up or down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "colab_type": "code",
    "id": "urD80lzGVv4S",
    "outputId": "dd61a7bc-d7c5-453c-a0b4-219fd1e85dc4"
   },
   "outputs": [],
   "source": [
    "content_path = IMAGEDIR + 'Bacchus.jpg'\n",
    "style_path = IMAGEDIR + 'Gleizes_The_Bridges_of_Paris.jpg'\n",
    "style_path_0 = IMAGEDIR + 'Blue_Water_Lilies_Monet.jpg'\n",
    "style_path_1 = IMAGEDIR + 'Delaunay_Window_on_the_City.jpg'\n",
    "style_path_2 = IMAGEDIR + 'Christ_in_Limbo.jpg'\n",
    "style_path_3 = IMAGEDIR + 'Kandinsky_Composition_7.jpg'\n",
    "style_path_4 = IMAGEDIR + 'Babel_Bruegel.jpg'\n",
    "style_path_5 = IMAGEDIR + 'Wreckers_Coast_of_Northumberland_JMWTurner.jpg'\n",
    "style_path_6 = IMAGEDIR + 'Water_Lily_Pond_Monet.jpg'\n",
    "style_path_7 = IMAGEDIR + 'Tondals_Vision.jpg'\n",
    "style_path_8 = IMAGEDIR + 'The_Triumph_of_Death_Bruegel.jpg'\n",
    "style_path_9 = IMAGEDIR + 'Metzinger_Two_Nudes.jpg'\n",
    "\n",
    "[c_w,fs_w,ms_w,cs_w,v_w] = [0,1,1,1,1]\n",
    "[c_p,fs_p,ms_p,cs_p,v_p] = [1,1,1,1,1]\n",
    "[lr,beta1,beta2,epochs]=[0.02,0.98,0.9999,10]\n",
    "newim, losses = build_tri_image( content_path_a, \n",
    "                                style_path, style_path_1, style_path_9, #fine, mid, coarse\n",
    "                                contentStart=True,\n",
    "                                lr=lr,beta1=beta1,beta2=beta2,epochs=epochs,\n",
    "                                weights=[c_w,fs_w,ms_w,cs_w,v_w],\n",
    "                                powers=[c_p,fs_p,ms_p,cs_p,v_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_fine_mid_coarse\n",
    "filename = 'Bacchus_Paris_Window_Metz.jpg'\n",
    "plt.imsave(OUTDIR+filename,np.array(newim[0]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1ZLJdFGL0_23",
    "OFCcz8vTV5Q4",
    "kCfZupSoqG44",
    "zky_OiHNZmWe",
    "Z9yGvZG5Zitz",
    "dSTdv7cWWHff",
    "rxK7wIYkcwYK",
    "pPFi5vm5bQA8"
   ],
   "name": "CubistNST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
